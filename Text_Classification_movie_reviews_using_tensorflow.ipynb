{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9rAEOUcW-38c"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = keras.datasets.imdb\n",
        "(train_data, train_labels) , (test_data, test_labels) =data.load_data(num_words= 88000)  # we are using here only first 10000 words because for convinence\n",
        "print(train_data[0]) # this will return a list which consists of integers which we humans cannot understand so we will convert them into words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IkTRw1qs_0H-",
        "outputId": "526ff360-1c43-4ddc-a379-c43fd4cbb615"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17464789/17464789 [==============================] - 1s 0us/step\n",
            "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mapping\n"
      ],
      "metadata": {
        "id": "Bm9_aaz0A2j8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_index= data.get_word_index() #this will return a tuple which consists of values\n",
        "word_index = {k:(v+3) for k,v in word_index.items()} # where k stands for keys and v stands for values\n",
        "word_index[\"<PAD>\"] = 0\n",
        "word_index[\"<START>\"] = 1\n",
        "word_index[\"<UNK>\"] = 2\n",
        "word_index[\"<UNUSED>\"] = 3\n",
        "\n",
        "reverse_word_index = dict([(value,key) for (key,value) in word_index.items()]) # reversing the word\n",
        "\n",
        "train_data = keras.preprocessing.sequence.pad_sequences(train_data, value =word_index[\"<PAD>\"],padding=\"post\",maxlen=250)\n",
        "test_data = keras.preprocessing.sequence.pad_sequences(test_data, value =word_index[\"<PAD>\"],padding=\"post\",maxlen=250)\n",
        "\n",
        "\n",
        "def decode_review(text):\n",
        "  return \" \".join([reverse_word_index.get(i, \"?\") for i in  text])\n",
        "\n",
        "print(decode_review(test_data[2]))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jc_keCp8A3_2",
        "outputId": "be28ad45-6fa5-4944-e694-d9e8f1567e04"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "1641221/1641221 [==============================] - 0s 0us/step\n",
            "king irritated <UNK> sends them a <UNK> br br delighted with this <UNK> looking new king who towers above them the <UNK> welcome him with a <UNK> of <UNK> dressed <UNK> the mayor steps forward to hand him the key to the <UNK> as <UNK> cameras record the event to everyone's horror the <UNK> promptly eats the mayor and then goes on a merry rampage <UNK> citizens at random a title card <UNK> reads news of the king's <UNK> throughout the kingdom when the now terrified <UNK> once more <UNK> <UNK> for help he loses his temper and <UNK> their community with lightning <UNK> the moral of our story delivered by a hapless frog just before he is eaten is let well enough alone br br considering the time period when this startling little film was made and considering the fact that it was made by a russian <UNK> at the height of that <UNK> country's civil war it would be easy to see this as a <UNK> about those events <UNK> may or may not have had <UNK> turmoil in mind when he made <UNK> but whatever <UNK> his choice of material the film stands as a <UNK> tale of universal <UNK> <UNK> could be the soviet union italy germany or japan in the 1930s or any country of any era that lets its guard down and is overwhelmed by <UNK> it's a fascinating film even a charming one in its macabre way but its message is no joke\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model\n",
        "\n"
      ],
      "metadata": {
        "id": "PDFqKAouiTXg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model =keras.Sequential()\n",
        "model.add(keras.layers.Embedding(88000,16))\n",
        "model.add(keras.layers.GlobalAveragePooling1D())\n",
        "model.add(keras.layers.Dense(16,activation=\"relu\"))\n",
        "model.add(keras.layers.Dense(1,activation=\"sigmoid\"))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(optimizer =\"adam\" , loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "x_val = train_data[:10000]\n",
        "x_train =train_data[10000:]\n",
        "\n",
        "y_val = train_labels[:10000]\n",
        "y_train =train_labels[10000:]\n",
        "\n",
        "fitModel = model.fit(x_train,y_train,epochs=40, batch_size=512, validation_data=(x_val,y_val),verbose=1)\n",
        "\n",
        "results = model.evaluate(test_data,test_labels)\n",
        "\n",
        "print(results)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "n04XVRagiYd2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "273ecbf0-394f-4866-e3ac-ff389af48b9f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, None, 16)          160000    \n",
            "                                                                 \n",
            " global_average_pooling1d (  (None, 16)                0         \n",
            " GlobalAveragePooling1D)                                         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 16)                272       \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 160289 (626.13 KB)\n",
            "Trainable params: 160289 (626.13 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/40\n",
            "30/30 [==============================] - 3s 71ms/step - loss: 0.6924 - accuracy: 0.5035 - val_loss: 0.6911 - val_accuracy: 0.4960\n",
            "Epoch 2/40\n",
            "30/30 [==============================] - 1s 36ms/step - loss: 0.6881 - accuracy: 0.5204 - val_loss: 0.6852 - val_accuracy: 0.5513\n",
            "Epoch 3/40\n",
            "30/30 [==============================] - 1s 37ms/step - loss: 0.6779 - accuracy: 0.5952 - val_loss: 0.6718 - val_accuracy: 0.6319\n",
            "Epoch 4/40\n",
            "30/30 [==============================] - 1s 39ms/step - loss: 0.6577 - accuracy: 0.6943 - val_loss: 0.6486 - val_accuracy: 0.7111\n",
            "Epoch 5/40\n",
            "30/30 [==============================] - 1s 33ms/step - loss: 0.6277 - accuracy: 0.7395 - val_loss: 0.6178 - val_accuracy: 0.7616\n",
            "Epoch 6/40\n",
            "30/30 [==============================] - 1s 33ms/step - loss: 0.5906 - accuracy: 0.7876 - val_loss: 0.5821 - val_accuracy: 0.7896\n",
            "Epoch 7/40\n",
            "30/30 [==============================] - 1s 35ms/step - loss: 0.5499 - accuracy: 0.8209 - val_loss: 0.5444 - val_accuracy: 0.8164\n",
            "Epoch 8/40\n",
            "30/30 [==============================] - 1s 33ms/step - loss: 0.5083 - accuracy: 0.8452 - val_loss: 0.5080 - val_accuracy: 0.8318\n",
            "Epoch 9/40\n",
            "30/30 [==============================] - 1s 38ms/step - loss: 0.4684 - accuracy: 0.8630 - val_loss: 0.4739 - val_accuracy: 0.8439\n",
            "Epoch 10/40\n",
            "30/30 [==============================] - 1s 35ms/step - loss: 0.4314 - accuracy: 0.8765 - val_loss: 0.4431 - val_accuracy: 0.8544\n",
            "Epoch 11/40\n",
            "30/30 [==============================] - 2s 51ms/step - loss: 0.3985 - accuracy: 0.8851 - val_loss: 0.4172 - val_accuracy: 0.8584\n",
            "Epoch 12/40\n",
            "30/30 [==============================] - 2s 54ms/step - loss: 0.3683 - accuracy: 0.8910 - val_loss: 0.3942 - val_accuracy: 0.8640\n",
            "Epoch 13/40\n",
            "30/30 [==============================] - 1s 45ms/step - loss: 0.3418 - accuracy: 0.8979 - val_loss: 0.3744 - val_accuracy: 0.8695\n",
            "Epoch 14/40\n",
            "30/30 [==============================] - 1s 35ms/step - loss: 0.3186 - accuracy: 0.9030 - val_loss: 0.3582 - val_accuracy: 0.8708\n",
            "Epoch 15/40\n",
            "30/30 [==============================] - 1s 27ms/step - loss: 0.2979 - accuracy: 0.9084 - val_loss: 0.3437 - val_accuracy: 0.8766\n",
            "Epoch 16/40\n",
            "30/30 [==============================] - 1s 21ms/step - loss: 0.2795 - accuracy: 0.9137 - val_loss: 0.3323 - val_accuracy: 0.8767\n",
            "Epoch 17/40\n",
            "30/30 [==============================] - 1s 22ms/step - loss: 0.2632 - accuracy: 0.9174 - val_loss: 0.3224 - val_accuracy: 0.8805\n",
            "Epoch 18/40\n",
            "30/30 [==============================] - 1s 19ms/step - loss: 0.2492 - accuracy: 0.9216 - val_loss: 0.3156 - val_accuracy: 0.8803\n",
            "Epoch 19/40\n",
            "30/30 [==============================] - 1s 21ms/step - loss: 0.2357 - accuracy: 0.9249 - val_loss: 0.3095 - val_accuracy: 0.8805\n",
            "Epoch 20/40\n",
            "30/30 [==============================] - 1s 19ms/step - loss: 0.2239 - accuracy: 0.9288 - val_loss: 0.3027 - val_accuracy: 0.8833\n",
            "Epoch 21/40\n",
            "30/30 [==============================] - 1s 21ms/step - loss: 0.2130 - accuracy: 0.9334 - val_loss: 0.2980 - val_accuracy: 0.8847\n",
            "Epoch 22/40\n",
            "30/30 [==============================] - 1s 19ms/step - loss: 0.2030 - accuracy: 0.9367 - val_loss: 0.2966 - val_accuracy: 0.8838\n",
            "Epoch 23/40\n",
            "30/30 [==============================] - 1s 18ms/step - loss: 0.1937 - accuracy: 0.9393 - val_loss: 0.2931 - val_accuracy: 0.8838\n",
            "Epoch 24/40\n",
            "30/30 [==============================] - 1s 21ms/step - loss: 0.1852 - accuracy: 0.9434 - val_loss: 0.2900 - val_accuracy: 0.8846\n",
            "Epoch 25/40\n",
            "30/30 [==============================] - 1s 19ms/step - loss: 0.1770 - accuracy: 0.9468 - val_loss: 0.2882 - val_accuracy: 0.8858\n",
            "Epoch 26/40\n",
            "30/30 [==============================] - 1s 21ms/step - loss: 0.1694 - accuracy: 0.9498 - val_loss: 0.2900 - val_accuracy: 0.8853\n",
            "Epoch 27/40\n",
            "30/30 [==============================] - 1s 19ms/step - loss: 0.1629 - accuracy: 0.9517 - val_loss: 0.2865 - val_accuracy: 0.8854\n",
            "Epoch 28/40\n",
            "30/30 [==============================] - 1s 30ms/step - loss: 0.1562 - accuracy: 0.9547 - val_loss: 0.2866 - val_accuracy: 0.8848\n",
            "Epoch 29/40\n",
            "30/30 [==============================] - 1s 29ms/step - loss: 0.1501 - accuracy: 0.9572 - val_loss: 0.2879 - val_accuracy: 0.8866\n",
            "Epoch 30/40\n",
            "30/30 [==============================] - 1s 32ms/step - loss: 0.1440 - accuracy: 0.9590 - val_loss: 0.2868 - val_accuracy: 0.8854\n",
            "Epoch 31/40\n",
            "30/30 [==============================] - 1s 23ms/step - loss: 0.1384 - accuracy: 0.9623 - val_loss: 0.2878 - val_accuracy: 0.8867\n",
            "Epoch 32/40\n",
            "30/30 [==============================] - 1s 21ms/step - loss: 0.1329 - accuracy: 0.9640 - val_loss: 0.2892 - val_accuracy: 0.8867\n",
            "Epoch 33/40\n",
            "30/30 [==============================] - 1s 18ms/step - loss: 0.1277 - accuracy: 0.9660 - val_loss: 0.2898 - val_accuracy: 0.8852\n",
            "Epoch 34/40\n",
            "30/30 [==============================] - 1s 21ms/step - loss: 0.1232 - accuracy: 0.9667 - val_loss: 0.2928 - val_accuracy: 0.8850\n",
            "Epoch 35/40\n",
            "30/30 [==============================] - 1s 21ms/step - loss: 0.1184 - accuracy: 0.9692 - val_loss: 0.2941 - val_accuracy: 0.8844\n",
            "Epoch 36/40\n",
            "30/30 [==============================] - 1s 19ms/step - loss: 0.1145 - accuracy: 0.9695 - val_loss: 0.2968 - val_accuracy: 0.8833\n",
            "Epoch 37/40\n",
            "30/30 [==============================] - 1s 21ms/step - loss: 0.1100 - accuracy: 0.9715 - val_loss: 0.2974 - val_accuracy: 0.8837\n",
            "Epoch 38/40\n",
            "30/30 [==============================] - 1s 21ms/step - loss: 0.1059 - accuracy: 0.9733 - val_loss: 0.3003 - val_accuracy: 0.8838\n",
            "Epoch 39/40\n",
            "30/30 [==============================] - 1s 19ms/step - loss: 0.1019 - accuracy: 0.9743 - val_loss: 0.3030 - val_accuracy: 0.8829\n",
            "Epoch 40/40\n",
            "30/30 [==============================] - 1s 18ms/step - loss: 0.0983 - accuracy: 0.9757 - val_loss: 0.3054 - val_accuracy: 0.8827\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.3268 - accuracy: 0.8714\n",
            "[0.3267626464366913, 0.8714399933815002]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reviews\n"
      ],
      "metadata": {
        "id": "Gbko5cFXGEZ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# test_review =test_data[0]\n",
        "# predict = model.predict([test_review])\n",
        "# print(\"Review :\")\n",
        "# print(decode_review(test_review))\n",
        "# print(\"Prediction:\" +str(predict[0]))\n",
        "# print(\"Actual :\" + str(test_labels[0]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 772
        },
        "id": "rvk2frtSGF3V",
        "outputId": "fc196c98-c060-4951-84ae-8b7c4922777a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-726350a4a88f>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtest_review\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpredict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_review\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Review :\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecode_review\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_review\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Prediction:\"\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mtf__predict_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2341, in predict_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2327, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2315, in run_step  **\n        outputs = model.predict_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2283, in predict_step\n        return self(x, training=False)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/input_spec.py\", line 235, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer 'sequential' (type Sequential).\n    \n    Input 0 of layer \"global_average_pooling1d\" is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, 16)\n    \n    Call arguments received by layer 'sequential' (type Sequential):\n      • inputs=('tf.Tensor(shape=(None,), dtype=int32)',)\n      • training=False\n      • mask=None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving the model\n"
      ],
      "metadata": {
        "id": "zaeiG6gpzs-a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"model.h5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fBT_cicizvBi",
        "outputId": "5ab70c50-5b1a-4ce7-8c61-ddb7f70b20cb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def review_encode(s):\n",
        "  encoded = [1]\n",
        "\n",
        "  for word in s:\n",
        "    if word.lower() in word_index:\n",
        "      encoded.append(word_index[word.lower()])\n",
        "    else:\n",
        "      encoded.append(2)\n",
        "  return encoded\n",
        "\n",
        "\n",
        "model = keras.models.load_model(\"model.h5\")\n",
        "\n",
        "\n",
        "with open(\"test1.txt\" , encoding =\"utf-8\") as f:\n",
        "  for line in f.readlines():\n",
        "    nline = line.replace( \",\", \"\").replace(\".\",\"\").replace(\"(\",\"\").replace(\")\",\"\").replace(\":\",\"\").replace(\"\\\"\",\"\").strip().split(\" \")\n",
        "    encode= review_encode(nline)\n",
        "    encode = keras.preprocessing.sequence.pad_sequences(train_data, value =word_index[\"<PAD>\"],padding=\"post\",maxlen=250)\n",
        "    predict=model.predict(encode)\n",
        "    print(line)\n",
        "    print(encode)\n",
        "    print(predict[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZwdmAOzd6YqU",
        "outputId": "870fd836-41fd-43a9-8fb8-50a4af7f8845"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 1s 2ms/step\n",
            "\"Past Lives\" is like a Stephen Sondheim song come to life.\n",
            "\n",
            "[[   1   14   22 ...    0    0    0]\n",
            " [   1  194 1153 ...    0    0    0]\n",
            " [   1   14   47 ...    0    0    0]\n",
            " ...\n",
            " [   1   11    6 ...    0    0    0]\n",
            " [   1 1446 7079 ...    0    0    0]\n",
            " [   1   17    6 ...    0    0    0]]\n",
            "[0.9868332]\n",
            "782/782 [==============================] - 1s 2ms/step\n",
            "\n",
            "\n",
            "[[   1   14   22 ...    0    0    0]\n",
            " [   1  194 1153 ...    0    0    0]\n",
            " [   1   14   47 ...    0    0    0]\n",
            " ...\n",
            " [   1   11    6 ...    0    0    0]\n",
            " [   1 1446 7079 ...    0    0    0]\n",
            " [   1   17    6 ...    0    0    0]]\n",
            "[0.9868332]\n",
            "782/782 [==============================] - 2s 2ms/step\n",
            "You know the song \"The Road You Didn't Take\" from \"Follies?\" How it's all about the different doors that exist along the path of life, but you only get to enter one at a time, leaving you to wonder what was behind the other doors. And that one path isn't necessarily better or worse than another, just different. And that you'll never know what was behind the doors you didn't choose, and that anyway time will eventually smooth over the regret you feel at not choosing them instead. Except that it doesn't always, and you may never stop completely regretting the life you think you might have had, even if you don't really mind the life you do have.\n",
            "\n",
            "[[   1   14   22 ...    0    0    0]\n",
            " [   1  194 1153 ...    0    0    0]\n",
            " [   1   14   47 ...    0    0    0]\n",
            " ...\n",
            " [   1   11    6 ...    0    0    0]\n",
            " [   1 1446 7079 ...    0    0    0]\n",
            " [   1   17    6 ...    0    0    0]]\n",
            "[0.9868332]\n",
            "782/782 [==============================] - 2s 2ms/step\n",
            "\n",
            "\n",
            "[[   1   14   22 ...    0    0    0]\n",
            " [   1  194 1153 ...    0    0    0]\n",
            " [   1   14   47 ...    0    0    0]\n",
            " ...\n",
            " [   1   11    6 ...    0    0    0]\n",
            " [   1 1446 7079 ...    0    0    0]\n",
            " [   1   17    6 ...    0    0    0]]\n",
            "[0.9868332]\n",
            "782/782 [==============================] - 1s 2ms/step\n",
            "This movie is that song. It's watching a beautifully defined female character convince herself and her husband that her life is what she wants it to be, and that she doesn't wish she'd taken a chance on the childhood sweetheart who comes back into her life after years apart and with whom she has off the charts chemistry. The actors who play the sweethearts as adults, Greta Lee and Teo Yoo, are both exceptional, but it's Lee who creates the most fascinating character. A bit cold, a bit distant, not always even completely likable in a way that's hard to define, a bit casually cruel to her husband who by anything the movie shows us can't be judged as anything other than a good, solid, and super understanding guy. But then there's that last scene that took my breath away, when we see the resolve not to feel emotions that would be inconvenient to feel give way, and we realize just how much has been roiling underneath the surface of Lee's character all along. It's magnificent and earned its right to leave me a quivering mess.\n",
            "\n",
            "[[   1   14   22 ...    0    0    0]\n",
            " [   1  194 1153 ...    0    0    0]\n",
            " [   1   14   47 ...    0    0    0]\n",
            " ...\n",
            " [   1   11    6 ...    0    0    0]\n",
            " [   1 1446 7079 ...    0    0    0]\n",
            " [   1   17    6 ...    0    0    0]]\n",
            "[0.9868332]\n",
            "782/782 [==============================] - 1s 2ms/step\n",
            "\n",
            "\n",
            "[[   1   14   22 ...    0    0    0]\n",
            " [   1  194 1153 ...    0    0    0]\n",
            " [   1   14   47 ...    0    0    0]\n",
            " ...\n",
            " [   1   11    6 ...    0    0    0]\n",
            " [   1 1446 7079 ...    0    0    0]\n",
            " [   1   17    6 ...    0    0    0]]\n",
            "[0.9868332]\n",
            "782/782 [==============================] - 1s 2ms/step\n",
            "The movie is also a sensitive look at what it feels like to be an immigrant, torn between two cultures, and not sure whether the nostalgia you feel is what you really want or just the allure of the familiar.\n",
            "\n",
            "[[   1   14   22 ...    0    0    0]\n",
            " [   1  194 1153 ...    0    0    0]\n",
            " [   1   14   47 ...    0    0    0]\n",
            " ...\n",
            " [   1   11    6 ...    0    0    0]\n",
            " [   1 1446 7079 ...    0    0    0]\n",
            " [   1   17    6 ...    0    0    0]]\n",
            "[0.9868332]\n",
            "782/782 [==============================] - 1s 2ms/step\n",
            "\n",
            "\n",
            "[[   1   14   22 ...    0    0    0]\n",
            " [   1  194 1153 ...    0    0    0]\n",
            " [   1   14   47 ...    0    0    0]\n",
            " ...\n",
            " [   1   11    6 ...    0    0    0]\n",
            " [   1 1446 7079 ...    0    0    0]\n",
            " [   1   17    6 ...    0    0    0]]\n",
            "[0.9868332]\n",
            "782/782 [==============================] - 2s 3ms/step\n",
            "A really great bit of counterprogramming in a summer that seems to be dishing up nothing but Marvel movies.\n",
            "\n",
            "[[   1   14   22 ...    0    0    0]\n",
            " [   1  194 1153 ...    0    0    0]\n",
            " [   1   14   47 ...    0    0    0]\n",
            " ...\n",
            " [   1   11    6 ...    0    0    0]\n",
            " [   1 1446 7079 ...    0    0    0]\n",
            " [   1   17    6 ...    0    0    0]]\n",
            "[0.9868332]\n",
            "782/782 [==============================] - 2s 2ms/step\n",
            "\n",
            "\n",
            "[[   1   14   22 ...    0    0    0]\n",
            " [   1  194 1153 ...    0    0    0]\n",
            " [   1   14   47 ...    0    0    0]\n",
            " ...\n",
            " [   1   11    6 ...    0    0    0]\n",
            " [   1 1446 7079 ...    0    0    0]\n",
            " [   1   17    6 ...    0    0    0]]\n",
            "[0.9868332]\n",
            "782/782 [==============================] - 1s 2ms/step\n",
            "Grade: A.\n",
            "[[   1   14   22 ...    0    0    0]\n",
            " [   1  194 1153 ...    0    0    0]\n",
            " [   1   14   47 ...    0    0    0]\n",
            " ...\n",
            " [   1   11    6 ...    0    0    0]\n",
            " [   1 1446 7079 ...    0    0    0]\n",
            " [   1   17    6 ...    0    0    0]]\n",
            "[0.9868332]\n"
          ]
        }
      ]
    }
  ]
}