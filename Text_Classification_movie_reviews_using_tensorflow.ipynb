{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "9rAEOUcW-38c"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = keras.datasets.imdb\n",
        "(train_data, train_labels) , (test_data, test_labels) =data.load_data(num_words= 88000)  # we are using here only first 10000 words because for convinence\n",
        "print(train_data[0]) # this will return a list which consists of integers which we humans cannot understand so we will convert them into words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IkTRw1qs_0H-",
        "outputId": "804253b1-a784-411b-ac96-bf14ce1f89ee"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 22665, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 21631, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 31050, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mapping\n"
      ],
      "metadata": {
        "id": "Bm9_aaz0A2j8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_index= data.get_word_index() #this will return a tuple which consists of values\n",
        "word_index = {k:(v+3) for k,v in word_index.items()} # where k stands for keys and v stands for values\n",
        "word_index[\"<PAD>\"] = 0\n",
        "word_index[\"<START>\"] = 1\n",
        "word_index[\"<UNK>\"] = 2\n",
        "word_index[\"<UNUSED>\"] = 3\n",
        "\n",
        "reverse_word_index = dict([(value,key) for (key,value) in word_index.items()]) # reversing the word\n",
        "\n",
        "train_data = keras.preprocessing.sequence.pad_sequences(train_data, value =word_index[\"<PAD>\"],padding=\"post\",maxlen=250)\n",
        "test_data = keras.preprocessing.sequence.pad_sequences(test_data, value =word_index[\"<PAD>\"],padding=\"post\",maxlen=250)\n",
        "\n",
        "\n",
        "def decode_review(text):\n",
        "  return \" \".join([reverse_word_index.get(i, \"?\") for i in  text])\n",
        "\n",
        "print(decode_review(test_data[2]))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jc_keCp8A3_2",
        "outputId": "af388c5f-0dc3-4d11-ac03-458991855838"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "king irritated jupiter sends them a stork br br delighted with this formidable looking new king who towers above them the frogs welcome him with a delegation of formally dressed dignitaries the mayor steps forward to hand him the key to the commonwealth as newsreel cameras record the event to everyone's horror the stork promptly eats the mayor and then goes on a merry rampage swallowing citizens at random a title card dryly reads news of the king's appetite throughout the kingdom when the now terrified frogs once more beseech jupiter for help he loses his temper and showers their community with lightning bolts the moral of our story delivered by a hapless frog just before he is eaten is let well enough alone br br considering the time period when this startling little film was made and considering the fact that it was made by a russian émigré at the height of that beleaguered country's civil war it would be easy to see this as a parable about those events starewicz may or may not have had russia's turmoil in mind when he made frogland but whatever prompted his choice of material the film stands as a cautionary tale of universal application frogland could be the soviet union italy germany or japan in the 1930s or any country of any era that lets its guard down and is overwhelmed by tyranny it's a fascinating film even a charming one in its macabre way but its message is no joke\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model\n",
        "\n"
      ],
      "metadata": {
        "id": "PDFqKAouiTXg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model =keras.Sequential()\n",
        "model.add(keras.layers.Embedding(88000,16))\n",
        "model.add(keras.layers.GlobalAveragePooling1D())\n",
        "model.add(keras.layers.Dense(16,activation=\"relu\"))\n",
        "model.add(keras.layers.Dense(1,activation=\"sigmoid\"))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(optimizer =\"adam\" , loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "x_val = train_data[:10000]\n",
        "x_train =train_data[10000:]\n",
        "\n",
        "y_val = train_labels[:10000]\n",
        "y_train =train_labels[10000:]\n",
        "\n",
        "fitModel = model.fit(x_train,y_train,epochs=40, batch_size=512, validation_data=(x_val,y_val),verbose=1)\n",
        "\n",
        "results = model.evaluate(test_data,test_labels)\n",
        "\n",
        "print(results)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "n04XVRagiYd2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e5bfd43-16c6-4b23-afca-a296fc804e65"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, None, 16)          1408000   \n",
            "                                                                 \n",
            " global_average_pooling1d_1  (None, 16)                0         \n",
            "  (GlobalAveragePooling1D)                                       \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 16)                272       \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1408289 (5.37 MB)\n",
            "Trainable params: 1408289 (5.37 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/40\n",
            "30/30 [==============================] - 2s 53ms/step - loss: 0.6923 - accuracy: 0.5420 - val_loss: 0.6910 - val_accuracy: 0.5947\n",
            "Epoch 2/40\n",
            "30/30 [==============================] - 1s 49ms/step - loss: 0.6884 - accuracy: 0.6181 - val_loss: 0.6857 - val_accuracy: 0.6560\n",
            "Epoch 3/40\n",
            "30/30 [==============================] - 1s 36ms/step - loss: 0.6791 - accuracy: 0.7491 - val_loss: 0.6736 - val_accuracy: 0.7308\n",
            "Epoch 4/40\n",
            "30/30 [==============================] - 1s 36ms/step - loss: 0.6605 - accuracy: 0.7789 - val_loss: 0.6516 - val_accuracy: 0.7686\n",
            "Epoch 5/40\n",
            "30/30 [==============================] - 1s 36ms/step - loss: 0.6301 - accuracy: 0.8010 - val_loss: 0.6194 - val_accuracy: 0.7870\n",
            "Epoch 6/40\n",
            "30/30 [==============================] - 1s 33ms/step - loss: 0.5886 - accuracy: 0.8130 - val_loss: 0.5800 - val_accuracy: 0.7998\n",
            "Epoch 7/40\n",
            "30/30 [==============================] - 1s 35ms/step - loss: 0.5397 - accuracy: 0.8349 - val_loss: 0.5363 - val_accuracy: 0.8167\n",
            "Epoch 8/40\n",
            "30/30 [==============================] - 1s 33ms/step - loss: 0.4888 - accuracy: 0.8534 - val_loss: 0.4942 - val_accuracy: 0.8282\n",
            "Epoch 9/40\n",
            "30/30 [==============================] - 1s 36ms/step - loss: 0.4398 - accuracy: 0.8690 - val_loss: 0.4554 - val_accuracy: 0.8381\n",
            "Epoch 10/40\n",
            "30/30 [==============================] - 1s 34ms/step - loss: 0.3952 - accuracy: 0.8834 - val_loss: 0.4220 - val_accuracy: 0.8461\n",
            "Epoch 11/40\n",
            "30/30 [==============================] - 1s 33ms/step - loss: 0.3559 - accuracy: 0.8945 - val_loss: 0.3945 - val_accuracy: 0.8558\n",
            "Epoch 12/40\n",
            "30/30 [==============================] - 1s 42ms/step - loss: 0.3224 - accuracy: 0.9036 - val_loss: 0.3715 - val_accuracy: 0.8647\n",
            "Epoch 13/40\n",
            "30/30 [==============================] - 1s 48ms/step - loss: 0.2932 - accuracy: 0.9122 - val_loss: 0.3535 - val_accuracy: 0.8662\n",
            "Epoch 14/40\n",
            "30/30 [==============================] - 1s 42ms/step - loss: 0.2684 - accuracy: 0.9179 - val_loss: 0.3379 - val_accuracy: 0.8729\n",
            "Epoch 15/40\n",
            "30/30 [==============================] - 1s 34ms/step - loss: 0.2458 - accuracy: 0.9260 - val_loss: 0.3254 - val_accuracy: 0.8758\n",
            "Epoch 16/40\n",
            "30/30 [==============================] - 1s 33ms/step - loss: 0.2265 - accuracy: 0.9327 - val_loss: 0.3154 - val_accuracy: 0.8779\n",
            "Epoch 17/40\n",
            "30/30 [==============================] - 1s 35ms/step - loss: 0.2094 - accuracy: 0.9386 - val_loss: 0.3081 - val_accuracy: 0.8807\n",
            "Epoch 18/40\n",
            "30/30 [==============================] - 1s 34ms/step - loss: 0.1939 - accuracy: 0.9430 - val_loss: 0.3007 - val_accuracy: 0.8826\n",
            "Epoch 19/40\n",
            "30/30 [==============================] - 1s 33ms/step - loss: 0.1798 - accuracy: 0.9483 - val_loss: 0.2946 - val_accuracy: 0.8830\n",
            "Epoch 20/40\n",
            "30/30 [==============================] - 1s 33ms/step - loss: 0.1667 - accuracy: 0.9525 - val_loss: 0.2901 - val_accuracy: 0.8845\n",
            "Epoch 21/40\n",
            "30/30 [==============================] - 1s 34ms/step - loss: 0.1552 - accuracy: 0.9576 - val_loss: 0.2861 - val_accuracy: 0.8857\n",
            "Epoch 22/40\n",
            "30/30 [==============================] - 1s 34ms/step - loss: 0.1443 - accuracy: 0.9618 - val_loss: 0.2832 - val_accuracy: 0.8869\n",
            "Epoch 23/40\n",
            "30/30 [==============================] - 1s 33ms/step - loss: 0.1344 - accuracy: 0.9654 - val_loss: 0.2807 - val_accuracy: 0.8872\n",
            "Epoch 24/40\n",
            "30/30 [==============================] - 1s 43ms/step - loss: 0.1253 - accuracy: 0.9690 - val_loss: 0.2794 - val_accuracy: 0.8871\n",
            "Epoch 25/40\n",
            "30/30 [==============================] - 1s 49ms/step - loss: 0.1169 - accuracy: 0.9722 - val_loss: 0.2773 - val_accuracy: 0.8878\n",
            "Epoch 26/40\n",
            "30/30 [==============================] - 1s 40ms/step - loss: 0.1091 - accuracy: 0.9752 - val_loss: 0.2769 - val_accuracy: 0.8884\n",
            "Epoch 27/40\n",
            "30/30 [==============================] - 1s 33ms/step - loss: 0.1019 - accuracy: 0.9775 - val_loss: 0.2763 - val_accuracy: 0.8884\n",
            "Epoch 28/40\n",
            "30/30 [==============================] - 1s 35ms/step - loss: 0.0954 - accuracy: 0.9791 - val_loss: 0.2764 - val_accuracy: 0.8887\n",
            "Epoch 29/40\n",
            "30/30 [==============================] - 1s 35ms/step - loss: 0.0891 - accuracy: 0.9817 - val_loss: 0.2781 - val_accuracy: 0.8881\n",
            "Epoch 30/40\n",
            "30/30 [==============================] - 1s 34ms/step - loss: 0.0837 - accuracy: 0.9828 - val_loss: 0.2766 - val_accuracy: 0.8903\n",
            "Epoch 31/40\n",
            "30/30 [==============================] - 1s 33ms/step - loss: 0.0782 - accuracy: 0.9845 - val_loss: 0.2775 - val_accuracy: 0.8907\n",
            "Epoch 32/40\n",
            "30/30 [==============================] - 1s 34ms/step - loss: 0.0732 - accuracy: 0.9864 - val_loss: 0.2784 - val_accuracy: 0.8898\n",
            "Epoch 33/40\n",
            "30/30 [==============================] - 1s 32ms/step - loss: 0.0688 - accuracy: 0.9868 - val_loss: 0.2794 - val_accuracy: 0.8889\n",
            "Epoch 34/40\n",
            "30/30 [==============================] - 1s 33ms/step - loss: 0.0646 - accuracy: 0.9879 - val_loss: 0.2815 - val_accuracy: 0.8898\n",
            "Epoch 35/40\n",
            "30/30 [==============================] - 1s 34ms/step - loss: 0.0606 - accuracy: 0.9893 - val_loss: 0.2825 - val_accuracy: 0.8894\n",
            "Epoch 36/40\n",
            "30/30 [==============================] - 1s 44ms/step - loss: 0.0569 - accuracy: 0.9901 - val_loss: 0.2842 - val_accuracy: 0.8891\n",
            "Epoch 37/40\n",
            "30/30 [==============================] - 2s 51ms/step - loss: 0.0536 - accuracy: 0.9911 - val_loss: 0.2876 - val_accuracy: 0.8886\n",
            "Epoch 38/40\n",
            "30/30 [==============================] - 1s 36ms/step - loss: 0.0510 - accuracy: 0.9913 - val_loss: 0.2881 - val_accuracy: 0.8885\n",
            "Epoch 39/40\n",
            "30/30 [==============================] - 1s 33ms/step - loss: 0.0476 - accuracy: 0.9926 - val_loss: 0.2919 - val_accuracy: 0.8880\n",
            "Epoch 40/40\n",
            "30/30 [==============================] - 1s 34ms/step - loss: 0.0449 - accuracy: 0.9928 - val_loss: 0.2921 - val_accuracy: 0.8886\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.3261 - accuracy: 0.8722\n",
            "[0.32609742879867554, 0.8722000122070312]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reviews\n"
      ],
      "metadata": {
        "id": "Gbko5cFXGEZ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# test_review =test_data[0]\n",
        "# predict = model.predict([test_review])\n",
        "# print(\"Review :\")\n",
        "# print(decode_review(test_review))\n",
        "# print(\"Prediction:\" +str(predict[0]))\n",
        "# print(\"Actual :\" + str(test_labels[0]))\n"
      ],
      "metadata": {
        "id": "rvk2frtSGF3V"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving the model\n"
      ],
      "metadata": {
        "id": "zaeiG6gpzs-a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"model.h5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fBT_cicizvBi",
        "outputId": "8b2cd9b2-1d3f-4a2a-fb91-f723d14f084f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def review_encode(s):\n",
        "  encoded = [1]\n",
        "\n",
        "  for word in s:\n",
        "    if word.lower() in word_index:\n",
        "      encoded.append(word_index[word.lower()])\n",
        "    else:\n",
        "      encoded.append(2)\n",
        "  return encoded\n",
        "\n",
        "\n",
        "model = keras.models.load_model(\"model.h5\")\n",
        "\n",
        "\n",
        "with open(\"test1.txt\" , encoding =\"utf-8\") as f:\n",
        "  for line in f.readlines():\n",
        "    nline = line.replace( \",\", \"\").replace(\".\",\"\").replace(\"(\",\"\").replace(\")\",\"\").replace(\":\",\"\").replace(\"\\\"\",\"\").strip().split(\" \")\n",
        "    encode= review_encode(nline)\n",
        "    encode = keras.preprocessing.sequence.pad_sequences(train_data, value =word_index[\"<PAD>\"],padding=\"post\",maxlen=250)\n",
        "    predict=model.predict(encode)\n",
        "    print(line)\n",
        "    print(encode)\n",
        "    print(predict[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZwdmAOzd6YqU",
        "outputId": "0d9bcb6f-dfca-4c27-aaf2-9ee62442c5f8"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 1s 2ms/step\n",
            "\"Past Lives\" is like a Stephen Sondheim song come to life.\n",
            "\n",
            "[[   1   14   22 ...    0    0    0]\n",
            " [   1  194 1153 ...    0    0    0]\n",
            " [   1   14   47 ...    0    0    0]\n",
            " ...\n",
            " [   1   11    6 ...    0    0    0]\n",
            " [   1 1446 7079 ...    0    0    0]\n",
            " [   1   17    6 ...    0    0    0]]\n",
            "[0.9990695]\n",
            "782/782 [==============================] - 2s 2ms/step\n",
            "\n",
            "\n",
            "[[   1   14   22 ...    0    0    0]\n",
            " [   1  194 1153 ...    0    0    0]\n",
            " [   1   14   47 ...    0    0    0]\n",
            " ...\n",
            " [   1   11    6 ...    0    0    0]\n",
            " [   1 1446 7079 ...    0    0    0]\n",
            " [   1   17    6 ...    0    0    0]]\n",
            "[0.9990695]\n",
            "782/782 [==============================] - 2s 2ms/step\n",
            "You know the song \"The Road You Didn't Take\" from \"Follies?\" How it's all about the different doors that exist along the path of life, but you only get to enter one at a time, leaving you to wonder what was behind the other doors. And that one path isn't necessarily better or worse than another, just different. And that you'll never know what was behind the doors you didn't choose, and that anyway time will eventually smooth over the regret you feel at not choosing them instead. Except that it doesn't always, and you may never stop completely regretting the life you think you might have had, even if you don't really mind the life you do have.\n",
            "\n",
            "[[   1   14   22 ...    0    0    0]\n",
            " [   1  194 1153 ...    0    0    0]\n",
            " [   1   14   47 ...    0    0    0]\n",
            " ...\n",
            " [   1   11    6 ...    0    0    0]\n",
            " [   1 1446 7079 ...    0    0    0]\n",
            " [   1   17    6 ...    0    0    0]]\n",
            "[0.9990695]\n",
            "782/782 [==============================] - 1s 2ms/step\n",
            "\n",
            "\n",
            "[[   1   14   22 ...    0    0    0]\n",
            " [   1  194 1153 ...    0    0    0]\n",
            " [   1   14   47 ...    0    0    0]\n",
            " ...\n",
            " [   1   11    6 ...    0    0    0]\n",
            " [   1 1446 7079 ...    0    0    0]\n",
            " [   1   17    6 ...    0    0    0]]\n",
            "[0.9990695]\n",
            "782/782 [==============================] - 1s 2ms/step\n",
            "This movie is that song. It's watching a beautifully defined female character convince herself and her husband that her life is what she wants it to be, and that she doesn't wish she'd taken a chance on the childhood sweetheart who comes back into her life after years apart and with whom she has off the charts chemistry. The actors who play the sweethearts as adults, Greta Lee and Teo Yoo, are both exceptional, but it's Lee who creates the most fascinating character. A bit cold, a bit distant, not always even completely likable in a way that's hard to define, a bit casually cruel to her husband who by anything the movie shows us can't be judged as anything other than a good, solid, and super understanding guy. But then there's that last scene that took my breath away, when we see the resolve not to feel emotions that would be inconvenient to feel give way, and we realize just how much has been roiling underneath the surface of Lee's character all along. It's magnificent and earned its right to leave me a quivering mess.\n",
            "\n",
            "[[   1   14   22 ...    0    0    0]\n",
            " [   1  194 1153 ...    0    0    0]\n",
            " [   1   14   47 ...    0    0    0]\n",
            " ...\n",
            " [   1   11    6 ...    0    0    0]\n",
            " [   1 1446 7079 ...    0    0    0]\n",
            " [   1   17    6 ...    0    0    0]]\n",
            "[0.9990695]\n",
            "782/782 [==============================] - 1s 2ms/step\n",
            "\n",
            "\n",
            "[[   1   14   22 ...    0    0    0]\n",
            " [   1  194 1153 ...    0    0    0]\n",
            " [   1   14   47 ...    0    0    0]\n",
            " ...\n",
            " [   1   11    6 ...    0    0    0]\n",
            " [   1 1446 7079 ...    0    0    0]\n",
            " [   1   17    6 ...    0    0    0]]\n",
            "[0.9990695]\n",
            "782/782 [==============================] - 1s 2ms/step\n",
            "The movie is also a sensitive look at what it feels like to be an immigrant, torn between two cultures, and not sure whether the nostalgia you feel is what you really want or just the allure of the familiar.\n",
            "\n",
            "[[   1   14   22 ...    0    0    0]\n",
            " [   1  194 1153 ...    0    0    0]\n",
            " [   1   14   47 ...    0    0    0]\n",
            " ...\n",
            " [   1   11    6 ...    0    0    0]\n",
            " [   1 1446 7079 ...    0    0    0]\n",
            " [   1   17    6 ...    0    0    0]]\n",
            "[0.9990695]\n",
            "782/782 [==============================] - 2s 2ms/step\n",
            "\n",
            "\n",
            "[[   1   14   22 ...    0    0    0]\n",
            " [   1  194 1153 ...    0    0    0]\n",
            " [   1   14   47 ...    0    0    0]\n",
            " ...\n",
            " [   1   11    6 ...    0    0    0]\n",
            " [   1 1446 7079 ...    0    0    0]\n",
            " [   1   17    6 ...    0    0    0]]\n",
            "[0.9990695]\n",
            "782/782 [==============================] - 2s 2ms/step\n",
            "A really great bit of counterprogramming in a summer that seems to be dishing up nothing but Marvel movies.\n",
            "\n",
            "[[   1   14   22 ...    0    0    0]\n",
            " [   1  194 1153 ...    0    0    0]\n",
            " [   1   14   47 ...    0    0    0]\n",
            " ...\n",
            " [   1   11    6 ...    0    0    0]\n",
            " [   1 1446 7079 ...    0    0    0]\n",
            " [   1   17    6 ...    0    0    0]]\n",
            "[0.9990695]\n",
            "782/782 [==============================] - 1s 2ms/step\n",
            "\n",
            "\n",
            "[[   1   14   22 ...    0    0    0]\n",
            " [   1  194 1153 ...    0    0    0]\n",
            " [   1   14   47 ...    0    0    0]\n",
            " ...\n",
            " [   1   11    6 ...    0    0    0]\n",
            " [   1 1446 7079 ...    0    0    0]\n",
            " [   1   17    6 ...    0    0    0]]\n",
            "[0.9990695]\n",
            "782/782 [==============================] - 1s 2ms/step\n",
            "Grade: A.\n",
            "[[   1   14   22 ...    0    0    0]\n",
            " [   1  194 1153 ...    0    0    0]\n",
            " [   1   14   47 ...    0    0    0]\n",
            " ...\n",
            " [   1   11    6 ...    0    0    0]\n",
            " [   1 1446 7079 ...    0    0    0]\n",
            " [   1   17    6 ...    0    0    0]]\n",
            "[0.9990695]\n"
          ]
        }
      ]
    }
  ]
}